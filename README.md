# Generative-Text-Model

*COMPANY*: CODTECH IT SOLUTIONS
*NAME*: SREEJA R
*INTERN ID*: CT06DR3163
*DOMAIN*: ARTIFICIAL INTELLIGENCE
*DURATION*: 6 WEEKS
*MENTOR*: NEELA SANTHOSH

**DESCRIPTION OF THE PROJECT**:

This project implements a *text generation system using the GPT-2 language model*, which is a transformer-based deep learning model developed for natural language processing tasks. The main objective of the project is to automatically generate meaningful and human-like text based on a given input prompt. GPT-2 is a pre-trained model that has learned language patterns, grammar, and context from a large corpus of text, enabling it to predict the next word in a sequence and generate coherent paragraphs.

In this project, the *Hugging Face Transformers* library is used to load the pre-trained GPT-2 model and its tokenizer. The tokenizer converts input text into numerical tokens that the model can understand, while the model processes these tokens to generate new text. PyTorch is used as the deep learning framework to handle tensor operations and model execution. The system automatically detects whether a GPU is available and runs on it if possible, otherwise it falls back to the CPU.

A text generation function is defined to take a user prompt as input and generate text using sampling techniques such as temperature scaling, top-k sampling, and nucleus (top-p) sampling. These parameters control creativity, randomness, and repetition in the generated output. The model produces a sequence of tokens, which are then decoded back into readable text. Overall, this project demonstrates how large language models can be used for applications such as content creation, story generation, chatbots, and creative writing, while providing a simple and practical introduction to transformer-based text generation.

**OUTPUT**:
<img width="731" height="68" alt="Image" src="https://github.com/user-attachments/assets/33abfeb7-7b26-4266-b28f-fb88392d8013" />
